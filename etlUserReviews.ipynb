{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\o0o0o0o\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\o0o0o0o\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('vader_lexicon')\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el json y lo guardamos en un string\n",
    "with open('src/australian_user_reviews.json', encoding='UTF-8') as f:\n",
    "    json_string = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazamos las comillas dobles por simples y luego las simples por dobles para que el json sea válido\n",
    "json_string = json_string.replace(\"\\\"\", \"'\")\n",
    "json_string = json_string.replace(\"\\\\'\", \"'\")\n",
    "json_string = json_string.replace(\"\\\\\", \"'\")\n",
    "json_string = json_string.replace(\"{'user_id': '\", '{\"user_id\": \"')\n",
    "json_string = json_string.replace(\"', 'user_url': '\", '\", \"user_url\": \"')\n",
    "json_string = json_string.replace(\"', 'reviews': [{'funny': '\", '\", \"reviews\": [{\"funny\": \"')\n",
    "json_string = json_string.replace(\"', 'reviews': []}\", '\", \"reviews\": []}')\n",
    "json_string = json_string.replace(\"'}, {'funny': '\", '\"}, {\"funny\": \"')\n",
    "json_string = json_string.replace(\"', 'posted': 'Posted \", '\", \"posted\": \"')\n",
    "json_string = json_string.replace(\"', 'last_edited': '\", '\", \"last_edited\": \"')\n",
    "json_string = json_string.replace(\"', 'item_id': '\", '\", \"item_id\": \"')\n",
    "json_string = json_string.replace(\"', 'helpful': '\", '\", \"helpful\": \"')\n",
    "json_string = json_string.replace(\"', 'recommend': True, 'review': '\", ' \", \"recommend\": \"True\", \"review\": \"')\n",
    "json_string = json_string.replace(\"', 'recommend': False, 'review': '\", ' \", \"recommend\": \"False\", \"review\": \"')\n",
    "json_string = json_string.replace(\"'}]}\", '\"}]}')  \n",
    "json_string = json_string.replace(\"\\\\xa0\", ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorremos el json y lo convertimos en un array. La columna reviews se desanida y se crea un nuevo array,\n",
    "# agregando el user_id a cada review\n",
    "data_array = []\n",
    "reviews_array = []\n",
    "for line in json_string.splitlines():\n",
    "    line = json.loads(line)\n",
    "    \n",
    "    if line['reviews'] != []:\n",
    "        for review in line['reviews']:\n",
    "                review[\"user_id\"] = line['user_id']\n",
    "                reviews_array.append(review)\n",
    "    data_array.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos los dataframes\n",
    "reviews_df = pd.DataFrame(reviews_array)\n",
    "users_df = pd.DataFrame(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiamos el df de reviews\n",
    "reviews_df = reviews_df.drop(columns=['funny', 'last_edited', 'helpful'])\n",
    "\n",
    "#Limpiamos el df de users\n",
    "users_df = users_df.drop(columns=['user_url', 'reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una función para extraer la fecha del texto, en los casos en los que no hay año se asume que son de 2016\n",
    "#ya que la última fecha con año es del 31 de diciembre de 2015\n",
    "def cleanYear(x):\n",
    "    try:\n",
    "        return datetime.strptime(x, '%B %d, %Y.')\n",
    "    except:\n",
    "        return datetime.strptime(x + '2016','%B %d.%Y')\n",
    "\n",
    "#Aplicamos la función al df de reviews y extraemos el año\n",
    "reviews_df['posted'] = reviews_df['posted'].apply(lambda x: cleanYear(x)).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acondicionamos el el texto de la columna review\n",
    "corpus = []\n",
    "for i in range(0, reviews_df.shape[0]):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', reviews_df['review'][i])# remplaza cualquier cosa que no sea Letras por espacios\n",
    "  review = review.lower()# pasar a minusculas\n",
    "  review = review.split() # se divide en plabras sin espacions\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')# eliminar palabras que no dan sentimiento\n",
    "  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregamos la columna tokenizada al df\n",
    "reviews_df['review_tokenized'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciamos el SentimentIntensityAnalyzer y lo aplicamos a la columna review_tokenized en una nueva columna\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "reviews_df['sentiment_analysis'] = reviews_df['review_tokenized'].apply(lambda x: round(sia.polarity_scores(x)['compound']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna review_tokenized y review y cambiamos el nombre de la columna sentiment por review\n",
    "reviews_df = reviews_df.drop(columns=['review_tokenized'])\n",
    "reviews_df = reviews_df.drop(columns=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos los tipos de las columnas \n",
    "\n",
    "reviews_df = reviews_df.convert_dtypes()\n",
    "reviews_df['item_id'] = reviews_df['item_id'].astype('int64')\n",
    "reviews_df['posted'] = reviews_df['posted'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cargamos el dataset de juegos y eliminamos las reviews sin juego asociado\n",
    "games_df = pd.read_parquet('src/cleaned/games.parquet')\n",
    "games_id_list = games_df['id'].tolist()\n",
    "\n",
    "reviews_df = reviews_df[reviews_df['item_id'].isin(games_id_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos los dataframes en archivos parquet\n",
    "reviews_df.to_parquet('src/cleaned/reviews.parquet', index=False)\n",
    "users_df.to_parquet('src/cleaned/users.parquet', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
